{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ต้องมาก่อนการใช้ TensorFlow ทุกอย่าง!\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "from river.drift import ADWIN  # นำเข้า ADWIN จากไลบรารี river\n",
    "\n",
    "# ===== ตั้งค่าความเสถียรและ reproducibility =====\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ปิด multi-threading ของ TensorFlow\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Data Preparation and Feature Engineering ---\n",
    "try:\n",
    "    df = pd.read_csv(\"nvidia_10yr_data.csv\", parse_dates=[\"Date\"])\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'nvidia_10yr_data.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "# Feature engineering\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df['Volatility'] = df['Close'].rolling(7).std()\n",
    "#df['Price_Diff'] = df['High'] - df['Low']\n",
    "df['Volume_Log'] = np.log1p(df['Volume'])\n",
    "# Feature interaction: Return * Volume_Log\n",
    "df['Return_Volume'] = df['Return'] * df['Volume_Log']\n",
    "#df['momentum_10'] = df['Close'] - df['Close'].shift(10)\n",
    "\n",
    "# Drop NaN after rolling calculations\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#X = df[['Return', 'Volatility', 'Volume_Log','Return_Volume']]\n",
    "X = df[['Return', 'Volatility','Volume_Log', 'Return_Volume']]\n",
    "#X = df[['Return','Volume_Log','Return_Volume']]\n",
    "y = df['Close']\n",
    "\n",
    "\n",
    "# --- 2. Model and Data Utility Classes ---\n",
    "class SequenceGenerator:\n",
    "    \"\"\"Class to create sequence data for RNN-based models and handle scaling.\"\"\"\n",
    "    def __init__(self, sequence_length: int = 30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "    def create_sequences(self, X: pd.DataFrame, y: pd.Series, fit_scalers: bool = True):\n",
    "        X_scaled = self.scaler_X.fit_transform(X) if fit_scalers else self.scaler_X.transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten() if fit_scalers else self.scaler_y.transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(self.sequence_length, len(X_scaled)):\n",
    "            X_seq.append(X_scaled[i-self.sequence_length:i])\n",
    "            y_seq.append(y_scaled[i])\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    def inverse_transform_y(self, y_scaled):\n",
    "        return self.scaler_y.inverse_transform(y_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "class RNNRegressor:\n",
    "    \"\"\"Universal RNN Regressor supporting SimpleRNN, LSTM, and GRU models.\"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', sequence_length: int = 30, units: int = 128, dropout_rate: float = 0.3, learning_rate: float = 0.0005, epochs: int = 200, batch_size: int = 32, verbose: int = 0):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.seq_generator = SequenceGenerator(sequence_length)\n",
    "        if self.model_type not in ['RNN', 'LSTM', 'GRU']:\n",
    "            raise ValueError(\"model_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "    def _get_layer_type(self):\n",
    "        if self.model_type == 'RNN': return SimpleRNN\n",
    "        elif self.model_type == 'LSTM': return LSTM\n",
    "        elif self.model_type == 'GRU': return GRU\n",
    "        \n",
    "    def _build_model(self, input_shape):\n",
    "        LayerType = self._get_layer_type()\n",
    "        model = Sequential([\n",
    "            LayerType(self.units, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(self.dropout_rate),\n",
    "            LayerType(self.units // 2, return_sequences=True), # เพิ่มเลเยอร์\n",
    "            Dropout(self.dropout_rate),\n",
    "            LayerType(self.units // 4),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        X_seq, y_seq = self.seq_generator.create_sequences(X, y, fit_scalers=True)\n",
    "        if len(X_seq) == 0: raise ValueError(\"Not enough data to create sequences\")\n",
    "        self.model = self._build_model((X_seq.shape[1], X_seq.shape[2]))\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True) # เพิ่ม patience\n",
    "        self.model.fit(X_seq, y_seq, epochs=self.epochs, batch_size=self.batch_size, callbacks=[early_stopping], verbose=self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        if self.model is None: raise ValueError(\"Model not fitted yet\")\n",
    "        # Ensure X is correctly transformed\n",
    "        X_scaled = self.seq_generator.scaler_X.transform(X)\n",
    "        X_seq, _ = self.seq_generator.create_sequences(X, pd.Series([0] * len(X)), fit_scalers=False)\n",
    "        if len(X_seq) == 0: return np.array([])\n",
    "        y_pred_scaled = self.model.predict(X_seq, verbose=0)\n",
    "        y_pred = self.seq_generator.inverse_transform_y(y_pred_scaled)\n",
    "        return y_pred\n",
    "\n",
    "class LinearRegressionModel:\n",
    "    \"\"\"Simple linear regression wrapper for compatibility.\"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.model = LinearRegression(fit_intercept=fit_intercept)\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        X_scaled = self.scaler_X.fit_transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        self.model.fit(X_scaled, y_scaled)\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        y_pred_scaled = self.model.predict(X_scaled)\n",
    "        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        return y_pred\n",
    "\n",
    "# --- 3. Concept Drift Detection (ใช้ ADWIN แทน) ---\n",
    "class ADWINDriftDetector:\n",
    "    \"\"\"Detects concept drift points using the ADWIN algorithm.\n",
    "    จะนับเฉพาะ drift ที่มีข้อมูลพอ และ merge fold สุดท้ายหากข้อมูลไม่พอ\"\"\"\n",
    "\n",
    "    def __init__(self, delta: float = 0.002, min_fold_len: int = 60):\n",
    "        self.detector = ADWIN(delta=delta)\n",
    "        self.min_fold_len = min_fold_len\n",
    "        self.drift_points_: List[int] = []\n",
    "\n",
    "    def detect(self, data: pd.DataFrame, target_column: str, seq_len: int = 30, test_ratio: float = 0.2) -> List[int]:\n",
    "        self.drift_points_ = []\n",
    "        series_to_monitor = data[target_column]\n",
    "        drift_points_temp = []\n",
    "        last_point = 0\n",
    "\n",
    "        # Step 1: Detect drift points\n",
    "        for i, val in enumerate(series_to_monitor):\n",
    "            self.detector.update(val)\n",
    "            if self.detector.drift_detected:\n",
    "                fold_len = i - last_point\n",
    "                split_idx = int((1-test_ratio)*fold_len)\n",
    "                train_len = split_idx\n",
    "                test_len = fold_len - split_idx\n",
    "                if fold_len >= self.min_fold_len and train_len >= seq_len and test_len >= seq_len:\n",
    "                    drift_points_temp.append(i)\n",
    "                    last_point = i\n",
    "                # ถ้าไม่พอข้อมูล ให้ข้าม drift นี้ไปเลย\n",
    "\n",
    "        # Step 2: พิจารณา fold สุดท้าย (ช่วง drift สุดท้าย -> len(data))\n",
    "        all_points = [0] + drift_points_temp + [len(data)]\n",
    "        last_drift_idx = len(drift_points_temp) - 1\n",
    "        prev_drift = drift_points_temp[last_drift_idx] if last_drift_idx >= 0 else 0\n",
    "        final_fold_len = len(data) - prev_drift\n",
    "        split_idx = int((1-test_ratio)*final_fold_len)\n",
    "        train_len = split_idx\n",
    "        test_len = final_fold_len - split_idx\n",
    "\n",
    "        # ถ้า fold สุดท้าย \"ไม่พอข้อมูล\" ให้ merge กับ drift ก่อนหน้า\n",
    "        if final_fold_len < self.min_fold_len or train_len < seq_len or test_len < seq_len:\n",
    "            # ลบ drift สุดท้ายออก (ถ้ามีมากกว่า 1 drift)\n",
    "            if len(drift_points_temp) > 0:\n",
    "                drift_points_temp = drift_points_temp[:-1]\n",
    "\n",
    "        self.drift_points_ = drift_points_temp\n",
    "        return self.drift_points_\n",
    "\n",
    "# --- 4. Cross-Validation Strategies ---\n",
    "\n",
    "class DriftAdaptiveTimeSeriesCV:\n",
    "    \"\"\"Performs cross-validation using a rolling window approach based on detected drift points.\n",
    "    แก้ไขให้ข้ามจุด drift ที่แบ่ง train/test ไม่ได้\"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]) -> Tuple[List[float], List[float]]:\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "        seq_len = self.model_params.get('sequence_length', 30)\n",
    "        min_fold_len = max(seq_len * 2, 40)\n",
    "        test_ratio = 0.2\n",
    "\n",
    "        all_points = sorted(list(set([0] + drift_points + [len(X)])))\n",
    "        for i in range(len(all_points) - 1):\n",
    "            start = all_points[i]\n",
    "            end = all_points[i+1]\n",
    "            fold_length = end - start\n",
    "            split_point = int(fold_length * (1 - test_ratio))\n",
    "            train_start = start\n",
    "            train_end = start + split_point\n",
    "            test_start = train_end\n",
    "            test_end = end\n",
    "\n",
    "            train_len = train_end - train_start\n",
    "            test_len = test_end - test_start\n",
    "\n",
    "            # เงื่อนไขสำหรับ Linear ไม่ต้องใช้ seq_len\n",
    "            if self.model_type == 'LINEAR':\n",
    "                if train_len <= 0 or test_len <= 0:\n",
    "                    print(f\"[Adaptive Fold {i+1}] Skipping (train/test < 1): train({train_len}), test({test_len})\")\n",
    "                    continue\n",
    "            else: # เงื่อนไขสำหรับ RNN/LSTM/GRU\n",
    "                if train_len <= seq_len or test_len <= seq_len:\n",
    "                    print(f\"[Adaptive Fold {i+1}] Skipping (train/test < seq_len): train({train_len}), test({test_len}), seq_len({seq_len})\")\n",
    "                    continue\n",
    "\n",
    "            split_point = int(fold_length * (1 - test_ratio))\n",
    "            train_start = start\n",
    "            train_end = start + split_point\n",
    "            test_start = train_end\n",
    "            test_end = end\n",
    "\n",
    "            train_len = train_end - train_start\n",
    "            test_len = test_end - test_start\n",
    "\n",
    "            # ตรวจสอบ train/test ต้องมีขนาดมากกว่า sequence_length\n",
    "            if train_len <= seq_len or test_len <= seq_len:\n",
    "                print(f\"[Adaptive Fold {i+1}] Skipping (train/test < seq_len): train({train_len}), test({test_len}), seq_len({seq_len})\")\n",
    "                continue\n",
    "\n",
    "            X_train, X_test = X.iloc[train_start:train_end], X.iloc[test_start:test_end]\n",
    "            y_train, y_test = y.iloc[train_start:train_end], y.iloc[test_start:test_end]\n",
    "\n",
    "            rmse, mae = np.nan, np.nan\n",
    "            if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "            elif self.model_type == 'LINEAR':\n",
    "                model = LinearRegressionModel(**self.model_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                    y_pred = y_pred[:len(y_test_aligned)]\n",
    "                else:\n",
    "                    y_test_aligned = y_test\n",
    "                if len(y_pred) > 0 and len(y_test_aligned) > 0:\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                    mae = mean_absolute_error(y_test_aligned, y_pred)\n",
    "                    metrics_rmse.append(rmse)\n",
    "                    metrics_mae.append(mae)\n",
    "                    print(f\"[Adaptive Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                else:\n",
    "                    print(f\"[Adaptive Fold {i+1}] Not enough data to calculate metrics.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Adaptive Fold {i+1}] Error: {e}\")\n",
    "\n",
    "        return metrics_rmse, metrics_mae\n",
    "\n",
    "class BaselineTimeSeriesCV:\n",
    "    \"\"\"\n",
    "    Performs cross-validation for time series data using a rolling window.\n",
    "    The data is split into n_splits + 1 parts.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None,\n",
    "                 n_splits: int = 4):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {}\n",
    "        self.n_splits = n_splits\n",
    "        if self.n_splits < 1:\n",
    "            raise ValueError(\"n_splits must be at least 1.\")\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[float], List[float]]:\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "        total_size = len(X)\n",
    "        \n",
    "        # Calculate the size of each part. The data is split into n_splits + 1 parts.\n",
    "        part_size = total_size // (self.n_splits + 1)\n",
    "        test_ratio = 0.2\n",
    "        \n",
    "        if part_size <= 0:\n",
    "            raise ValueError(\"Not enough data for the specified number of splits.\")\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            # กำหนดช่วงข้อมูลสำหรับ Fold ปัจจุบัน (ข้อมูลส่วนที่ i+1)\n",
    "            fold_start = i * part_size\n",
    "            fold_end = (i + 1) * part_size\n",
    "            if i == self.n_splits - 1: # จัดการส่วนสุดท้ายที่อาจมีขนาดไม่เท่ากัน\n",
    "                fold_end = total_size\n",
    "\n",
    "            fold_data_X = X.iloc[fold_start:fold_end]\n",
    "            fold_data_y = y.iloc[fold_start:fold_end]\n",
    "\n",
    "            # แบ่งชุดข้อมูลภายใน Fold เป็น Train และ Test\n",
    "            split_point = int(len(fold_data_X) * (1 - test_ratio))\n",
    "            \n",
    "            X_train = fold_data_X.iloc[:split_point]\n",
    "            y_train = fold_data_y.iloc[:split_point]\n",
    "            \n",
    "            X_test = fold_data_X.iloc[split_point:]\n",
    "            y_test = fold_data_y.iloc[split_point:]\n",
    "\n",
    "            # Check for sufficient data size for RNN-based models\n",
    "            seq_len = self.model_params.get('sequence_length', 30) if self.model_type != 'LINEAR' else 0\n",
    "            train_len = len(X_train)\n",
    "            test_len = len(X_test)\n",
    "            \n",
    "            if train_len <= seq_len or test_len <= seq_len:\n",
    "                print(f\"[Baseline Fold {i+1}] Skipping (train/test < seq_len): train({train_len}), test({test_len}), seq_len({seq_len})\")\n",
    "                continue\n",
    "\n",
    "            rmse, mae = np.nan, np.nan\n",
    "            try:\n",
    "                # Model instantiation and fitting\n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "                elif self.model_type == 'LINEAR':\n",
    "                    model = LinearRegressionModel(**self.model_params)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Align prediction and test data length for RNN models\n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    y_test_aligned = y_test.iloc[seq_len:]\n",
    "                else:\n",
    "                    y_test_aligned = y_test\n",
    "                \n",
    "                min_len = min(len(y_pred), len(y_test_aligned))\n",
    "                if min_len > 0:\n",
    "                    y_pred_trimmed = y_pred[:min_len]\n",
    "                    y_test_trimmed = y_test_aligned[:min_len]\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_trimmed, y_pred_trimmed))\n",
    "                    mae = mean_absolute_error(y_test_trimmed, y_pred_trimmed)\n",
    "                    metrics_rmse.append(rmse)\n",
    "                    metrics_mae.append(mae)\n",
    "                    print(f\"[Baseline Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                else:\n",
    "                    print(f\"[Baseline Fold {i+1}] Not enough data to calculate metrics.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Baseline Fold {i+1}] Error during model training/prediction: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return metrics_rmse, metrics_mae\n",
    "    \n",
    "# --- 5. Model Comparison and Execution ---\n",
    "\n",
    "class ModelComparison:\n",
    "    \"\"\"Compares the performance of different models using two CV strategies.\"\"\"\n",
    "    def __init__(self, rnn_params: dict = None, linear_params: dict = None):\n",
    "        self.rnn_params = rnn_params or {}\n",
    "        self.linear_params = linear_params or {}\n",
    "        self.models = ['RNN', 'LSTM', 'GRU', 'LINEAR']\n",
    "        \n",
    "    def compare_models(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]):\n",
    "        results = {}\n",
    "\n",
    "        for model_type in self.models:\n",
    "            params = self.linear_params if model_type == 'LINEAR' else self.rnn_params\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Testing {model_type} Model\")\n",
    "            print(f\"{'='*50}\")\n",
    "            # Adaptive CV ใช้จุด drift ทั้งหมดที่ตรวจพบ\n",
    "            drift_cv = DriftAdaptiveTimeSeriesCV(model_type, params)\n",
    "            drift_rmse, drift_mae = drift_cv.run(X, y, drift_points)\n",
    "            \n",
    "            # Baseline CV ใช้การแบ่ง 5-fold แบบปกติ\n",
    "            baseline_cv = BaselineTimeSeriesCV(model_type, params, n_splits=5)\n",
    "            base_rmse, base_mae = baseline_cv.run(X, y)\n",
    "            \n",
    "            results[model_type] = {'adaptive_rmse': drift_rmse, 'adaptive_mae': drift_mae, 'baseline_rmse': base_rmse, 'baseline_mae': base_mae}\n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: dict):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                print(f\"\\n{model_type} Results:\")\n",
    "                print(\"-\" * 30)\n",
    "                if results[model_type]['adaptive_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['adaptive_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['adaptive_mae'])\n",
    "                    print(f\"Adaptive CV - Avg RMSE: {avg_rmse:.3f}, Avg MAE: {avg_mae:.3f} \")\n",
    "                else:\n",
    "                    print(\"Adaptive CV - No valid results\")\n",
    "                if results[model_type]['baseline_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['baseline_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['baseline_mae'])\n",
    "                    print(f\"Baseline CV - Avg RMSE: {avg_rmse:.3f}, Avg MAE: {avg_mae:.3f}\")\n",
    "                else:\n",
    "                    print(\"Baseline CV - No valid results\")\n",
    "        best_model = self._find_best_model(results)\n",
    "        if best_model:\n",
    "            print(f\"\\nBest Model: {best_model}\")\n",
    "    \n",
    "    def _find_best_model(self, results: dict):\n",
    "        best_model, best_score = None, float('inf')\n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                scores = results[model_type]['adaptive_rmse'] or results[model_type]['baseline_rmse']\n",
    "                if scores and np.mean(scores) < best_score:\n",
    "                    best_score = np.mean(scores)\n",
    "                    best_model = model_type\n",
    "        return best_model\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    detector = ADWINDriftDetector(delta=0.01, min_fold_len=15)\n",
    "    drift_points = detector.detect(df, 'Close')\n",
    "    drift_dates_formatted = df.iloc[drift_points]['Date'].dt.strftime('%d/%m/%Y').tolist()\n",
    "    print(\"\\nDrift Dates:\")\n",
    "    print(drift_points)\n",
    "    print(drift_dates_formatted)\n",
    "    print(\"Len :\", len(drift_dates_formatted))\n",
    "    \n",
    "    # แก้ไข parameters ของ RNN\n",
    "    \n",
    "    rnn_params = {'sequence_length': 15, 'units': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 32, 'verbose': 0}\n",
    "    linear_params = {'fit_intercept': True}\n",
    "    comparator = ModelComparison(rnn_params=rnn_params, linear_params=linear_params)\n",
    "    results = comparator.compare_models(X, y, drift_points)\n",
    "    comparator.print_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df697d",
   "metadata": {},
   "source": [
    "# Show Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a996035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ใช้ข้อมูล Date กับ Close price สำหรับกราฟหลัก\n",
    "dates = df['Date']\n",
    "prices = df['Close']\n",
    "\n",
    "# Map drift point index เป็นวันที่\n",
    "drift_dates = df.iloc[drift_points]['Date'].values\n",
    "\n",
    "# Plot กราฟราคาพร้อมเส้นแสดง drift point\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(x=dates, y=prices, label='Close Price', color='blue')\n",
    "\n",
    "# วาดเส้นแนวตั้งสำหรับแต่ละ drift point\n",
    "for d in drift_dates:\n",
    "    plt.axvline(x=d, color='red', linestyle='-', alpha=0.7)\n",
    "\n",
    "# ตกแต่งกราฟ\n",
    "plt.title(\"Drift Points Detected in META Closing Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# แปลง drift point index เป็นวันที่แบบ วัน/เดือน/ปี\n",
    "drift_dates_formatted = df.iloc[drift_points]['Date'].dt.strftime('%d/%m/%Y').tolist()\n",
    "print(\"Drift Dates of META :\")\n",
    "print(drift_dates_formatted)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

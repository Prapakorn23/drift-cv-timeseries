{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57368dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drift Detection...\n",
      "Detected 8 drift points\n",
      "\n",
      "Starting Enhanced Model Comparison with Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "STARTING HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for RNN...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for RNN...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 0.0312\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "New best score: 0.0312\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 0.0312\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 0.0312\n",
      "Best parameters: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "[TUNING] Completed tuning for RNN\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for LSTM...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for LSTM...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "New best score: 0.0593\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 0.0319\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "New best score: 0.0311\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "New best score: 0.0311\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "New best score: 0.0311\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 0.0311\n",
      "Best parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "[TUNING] Completed tuning for LSTM\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for GRU...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for GRU...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 0.0312\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 0.0312\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 0.0311\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 0.0311\n",
      "Best parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "[TUNING] Completed tuning for GRU\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETED\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Testing RNN Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[Adaptive Fold 1] Training RNN...\n",
      "[Adaptive Fold 1] RMSE=0.019, MAE=0.014\n",
      "\n",
      "[Adaptive Fold 2] Training RNN...\n",
      "[Adaptive Fold 2] RMSE=0.023, MAE=0.018\n",
      "\n",
      "[Adaptive Fold 3] Training RNN...\n",
      "[Adaptive Fold 3] RMSE=0.025, MAE=0.019\n",
      "\n",
      "[Adaptive Fold 4] Training RNN...\n",
      "[Adaptive Fold 4] RMSE=0.025, MAE=0.022\n",
      "\n",
      "[Baseline Fold 1] Training RNN...\n",
      "[Baseline Fold 1] RMSE=0.027, MAE=0.019\n",
      "\n",
      "[Baseline Fold 2] Training RNN...\n",
      "[Baseline Fold 2] RMSE=0.030, MAE=0.021\n",
      "\n",
      "[Baseline Fold 3] Training RNN...\n",
      "[Baseline Fold 3] RMSE=0.029, MAE=0.022\n",
      "\n",
      "[Baseline Fold 4] Training RNN...\n",
      "[Baseline Fold 4] RMSE=0.038, MAE=0.030\n",
      "\n",
      "[Baseline Fold 5] Training RNN...\n",
      "[Baseline Fold 5] RMSE=0.030, MAE=0.023\n",
      "\n",
      "==================================================\n",
      "Testing LSTM Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[Adaptive Fold 1] Training LSTM...\n",
      "[Adaptive Fold 1] RMSE=0.021, MAE=0.015\n",
      "\n",
      "[Adaptive Fold 2] Training LSTM...\n",
      "[Adaptive Fold 2] RMSE=0.028, MAE=0.022\n",
      "\n",
      "[Adaptive Fold 3] Training LSTM...\n",
      "[Adaptive Fold 3] RMSE=0.024, MAE=0.019\n",
      "\n",
      "[Adaptive Fold 4] Training LSTM...\n",
      "[Adaptive Fold 4] RMSE=0.021, MAE=0.017\n",
      "\n",
      "[Baseline Fold 1] Training LSTM...\n",
      "[Baseline Fold 1] RMSE=0.026, MAE=0.018\n",
      "\n",
      "[Baseline Fold 2] Training LSTM...\n",
      "[Baseline Fold 2] RMSE=0.030, MAE=0.021\n",
      "\n",
      "[Baseline Fold 3] Training LSTM...\n",
      "[Baseline Fold 3] RMSE=0.030, MAE=0.022\n",
      "\n",
      "[Baseline Fold 4] Training LSTM...\n",
      "[Baseline Fold 4] RMSE=0.038, MAE=0.030\n",
      "\n",
      "[Baseline Fold 5] Training LSTM...\n",
      "[Baseline Fold 5] RMSE=0.030, MAE=0.023\n",
      "\n",
      "==================================================\n",
      "Testing GRU Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[Adaptive Fold 1] Training GRU...\n",
      "[Adaptive Fold 1] RMSE=0.020, MAE=0.015\n",
      "\n",
      "[Adaptive Fold 2] Training GRU...\n",
      "[Adaptive Fold 2] RMSE=0.023, MAE=0.018\n",
      "\n",
      "[Adaptive Fold 3] Training GRU...\n",
      "[Adaptive Fold 3] RMSE=0.024, MAE=0.019\n",
      "\n",
      "[Adaptive Fold 4] Training GRU...\n",
      "[Adaptive Fold 4] RMSE=0.022, MAE=0.019\n",
      "\n",
      "[Baseline Fold 1] Training GRU...\n",
      "[Baseline Fold 1] RMSE=0.026, MAE=0.018\n",
      "\n",
      "[Baseline Fold 2] Training GRU...\n",
      "[Baseline Fold 2] RMSE=0.030, MAE=0.021\n",
      "\n",
      "[Baseline Fold 3] Training GRU...\n",
      "[Baseline Fold 3] RMSE=0.029, MAE=0.022\n",
      "\n",
      "[Baseline Fold 4] Training GRU...\n",
      "[Baseline Fold 4] RMSE=0.038, MAE=0.030\n",
      "\n",
      "[Baseline Fold 5] Training GRU...\n",
      "[Baseline Fold 5] RMSE=0.030, MAE=0.023\n",
      "\n",
      "==================================================\n",
      "Testing LINEAR Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'fit_intercept': True}\n",
      "\n",
      "[Adaptive Fold 1] Training LINEAR...\n",
      "[Adaptive Fold 1] RMSE=0.001, MAE=0.001\n",
      "\n",
      "[Adaptive Fold 2] Training LINEAR...\n",
      "[Adaptive Fold 2] RMSE=0.001, MAE=0.000\n",
      "\n",
      "[Adaptive Fold 3] Training LINEAR...\n",
      "[Adaptive Fold 3] RMSE=0.000, MAE=0.000\n",
      "\n",
      "[Adaptive Fold 4] Training LINEAR...\n",
      "[Adaptive Fold 4] RMSE=0.001, MAE=0.000\n",
      "\n",
      "[Baseline Fold 1] Training LINEAR...\n",
      "[Baseline Fold 1] RMSE=0.002, MAE=0.001\n",
      "\n",
      "[Baseline Fold 2] Training LINEAR...\n",
      "[Baseline Fold 2] RMSE=0.002, MAE=0.001\n",
      "\n",
      "[Baseline Fold 3] Training LINEAR...\n",
      "[Baseline Fold 3] RMSE=0.001, MAE=0.001\n",
      "\n",
      "[Baseline Fold 4] Training LINEAR...\n",
      "[Baseline Fold 4] RMSE=0.001, MAE=0.001\n",
      "\n",
      "[Baseline Fold 5] Training LINEAR...\n",
      "[Baseline Fold 5] RMSE=0.002, MAE=0.001\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY WITH OPTIMIZED PARAMETERS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING RESULTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - RNN\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 0.0312\n",
      "Best parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.01\n",
      "  batch_size: 16\n",
      "  units: 32\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.01\n",
      "   batch_size: 16\n",
      "   units: 32\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "2. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.01\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 50\n",
      "\n",
      "3. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 16\n",
      "   units: 32\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 30\n",
      "\n",
      "4. Score: 0.0313\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 70\n",
      "\n",
      "5. Score: 0.0314\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 16\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 70\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - LSTM\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 0.0311\n",
      "Best parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.05\n",
      "  batch_size: 64\n",
      "  units: 32\n",
      "  dropout_rate: 0.2\n",
      "  epochs: 70\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 0.0311\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 32\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 70\n",
      "\n",
      "2. Score: 0.0311\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 70\n",
      "\n",
      "3. Score: 0.0311\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 50\n",
      "\n",
      "4. Score: 0.0311\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.01\n",
      "   batch_size: 16\n",
      "   units: 64\n",
      "   dropout_rate: 0.3\n",
      "   epochs: 50\n",
      "\n",
      "5. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 16\n",
      "   units: 64\n",
      "   dropout_rate: 0.3\n",
      "   epochs: 70\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - GRU\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 0.0311\n",
      "Best parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.05\n",
      "  batch_size: 32\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 0.0311\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 32\n",
      "   units: 64\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "2. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 30\n",
      "\n",
      "3. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 16\n",
      "   units: 32\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 30\n",
      "\n",
      "4. Score: 0.0312\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 32\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "5. Score: 0.0313\n",
      "   sequence_length: 45\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 64\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 30\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "RNN Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "Adaptive CV - Avg RMSE: 0.0231, Avg MAE: 0.0182\n",
      "Baseline CV - Avg RMSE: 0.0308, Avg MAE: 0.0228\n",
      "\n",
      "LSTM Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "Adaptive CV - Avg RMSE: 0.0235, Avg MAE: 0.0183\n",
      "Baseline CV - Avg RMSE: 0.0308, Avg MAE: 0.0228\n",
      "\n",
      "GRU Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "Adaptive CV - Avg RMSE: 0.0223, Avg MAE: 0.0178\n",
      "Baseline CV - Avg RMSE: 0.0307, Avg MAE: 0.0227\n",
      "\n",
      "LINEAR Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'fit_intercept': True}\n",
      "Adaptive CV - Avg RMSE: 0.0006, Avg MAE: 0.0004\n",
      "Baseline CV - Avg RMSE: 0.0016, Avg MAE: 0.0009\n",
      "\n",
      "============================================================\n",
      "MODEL RANKING (Based on Adaptive CV RMSE)\n",
      "============================================================\n",
      "\n",
      "1. LINEAR\n",
      "   RMSE: 0.0006\n",
      "   MAE: 0.0004\n",
      "   Best Parameters: {'fit_intercept': True}\n",
      "\n",
      "2. GRU\n",
      "   RMSE: 0.0223\n",
      "   MAE: 0.0178\n",
      "   Best Parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "3. RNN\n",
      "   RMSE: 0.0231\n",
      "   MAE: 0.0182\n",
      "   Best Parameters: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "4. LSTM\n",
      "   RMSE: 0.0235\n",
      "   MAE: 0.0183\n",
      "   Best Parameters: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "============================================================\n",
      "üèÜ BEST MODEL: LINEAR\n",
      "   RMSE: 0.0006\n",
      "   Optimal Parameters: {'fit_intercept': True}\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "BEST PARAMETERS FOR EACH MODEL\n",
      "================================================================================\n",
      "\n",
      "RNN Best Parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.01\n",
      "  batch_size: 16\n",
      "  units: 32\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "LSTM Best Parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.05\n",
      "  batch_size: 64\n",
      "  units: 32\n",
      "  dropout_rate: 0.2\n",
      "  epochs: 70\n",
      "\n",
      "GRU Best Parameters:\n",
      "  sequence_length: 45\n",
      "  learning_rate: 0.05\n",
      "  batch_size: 32\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "LINEAR Best Parameters:\n",
      "  fit_intercept: True\n",
      "\n",
      "================================================================================\n",
      "DRIFT DETECTION RESULTS\n",
      "================================================================================\n",
      "Detected Drift Points: 8\n",
      "Drift Dates:\n",
      "  1. 09/07/2015\n",
      "  2. 15/09/2016\n",
      "  3. 29/08/2017\n",
      "  4. 14/06/2019\n",
      "  5. 28/05/2020\n",
      "  6. 29/10/2021\n",
      "  7. 13/10/2022\n",
      "  8. 28/09/2023\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ TensorFlow ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á!\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "\n",
    "# ===== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞ reproducibility =====\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ‡∏õ‡∏¥‡∏î multi-threading ‡∏Ç‡∏≠‡∏á TensorFlow\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocess data\n",
    "df = pd.read_csv(\"nvidia_10yr_data.csv\", parse_dates=[\"Date\"])\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "# Feature engineering\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df['Volatility'] = df['Close'].rolling(10).std()\n",
    "df['Price_Diff'] = df['High'] - df['Low']\n",
    "df['Volume_Log'] = np.log1p(df['Volume'])\n",
    "df['LogReturn'] = np.log(df['Close']).diff()\n",
    "\n",
    "# Drop NaN ‡∏´‡∏•‡∏±‡∏á rolling\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df[['Return', 'Volatility', 'Price_Diff', 'Volume_Log']]\n",
    "y = df['LogReturn']\n",
    "\n",
    "class SequenceGenerator:\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á sequence data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN-based models\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length: int = 30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "    def create_sequences(self, X: pd.DataFrame, y: pd.Series, fit_scalers: bool = True):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡πâ‡∏≤‡∏á sequence data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN-based models\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        if fit_scalers:\n",
    "            X_scaled = self.scaler_X.fit_transform(X)\n",
    "            y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            X_scaled = self.scaler_X.transform(X)\n",
    "            y_scaled = self.scaler_y.transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create sequences\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(self.sequence_length, len(X_scaled)):\n",
    "            X_seq.append(X_scaled[i-self.sequence_length:i])\n",
    "            y_seq.append(y_scaled[i])\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    def inverse_transform_y(self, y_scaled):\n",
    "        \"\"\"\n",
    "        ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ y ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏Å‡∏•‡πÄ‡∏î‡∏¥‡∏°\n",
    "        \"\"\"\n",
    "        return self.scaler_y.inverse_transform(y_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "class RNNRegressor:\n",
    "    \"\"\"\n",
    "    Universal RNN Regressor ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö RNN, LSTM, ‡πÅ‡∏•‡∏∞ GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', sequence_length: int = 30, \n",
    "                 units: int = 50, dropout_rate: float = 0.2, \n",
    "                 learning_rate: float = 0.01, epochs: int = 100, \n",
    "                 batch_size: int = 32, verbose: int = 0):\n",
    "        \n",
    "        self.model_type = model_type.upper()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.seq_generator = SequenceGenerator(sequence_length)\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model_type ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        if self.model_type not in ['RNN', 'LSTM', 'GRU']:\n",
    "            raise ValueError(\"model_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "    def _get_layer_type(self):\n",
    "        \"\"\"\n",
    "        ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å layer type ‡∏ï‡∏≤‡∏° model_type\n",
    "        \"\"\"\n",
    "        if self.model_type == 'RNN':\n",
    "            return SimpleRNN\n",
    "        elif self.model_type == 'LSTM':\n",
    "            return LSTM\n",
    "        elif self.model_type == 'GRU':\n",
    "            return GRU\n",
    "        \n",
    "    def _build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• RNN ‡∏ï‡∏≤‡∏° model_type\n",
    "        \"\"\"\n",
    "        LayerType = self._get_layer_type()\n",
    "        \n",
    "        model = Sequential([\n",
    "            LayerType(self.units, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(self.dropout_rate),\n",
    "            LayerType(self.units // 2, return_sequences=False),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Train RNN model\n",
    "        \"\"\"\n",
    "        # Create sequences\n",
    "        X_seq, y_seq = self.seq_generator.create_sequences(X, y, fit_scalers=True)\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"Not enough data to create sequences\")\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_model((X_seq.shape[1], X_seq.shape[2]))\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(\n",
    "            X_seq, y_seq,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Create sequences (don't fit scalers)\n",
    "        X_seq, _ = self.seq_generator.create_sequences(\n",
    "            X, pd.Series([0] * len(X)), fit_scalers=False\n",
    "        )\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            # Return predictions for available data points\n",
    "            return np.array([])\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_scaled = self.model.predict(X_seq, verbose=0)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_pred = self.seq_generator.inverse_transform_y(y_pred_scaled)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "class LinearRegressionModel:\n",
    "    \"\"\"\n",
    "    Linear Regression model with standardization\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept: bool = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self.model = LinearRegression(fit_intercept=fit_intercept)\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Train Linear Regression model\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler_X.fit_transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_scaled, y_scaled)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_scaled = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', n_splits: int = 3):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.n_splits = n_splits\n",
    "        self.best_params = {}\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "        \n",
    "    def define_param_grid(self):\n",
    "        \"\"\"\n",
    "        ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ tuning\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'sequence_length': [20, 30, 45],\n",
    "            'learning_rate': [0.001, 0.01, 0.05],\n",
    "            'batch_size': [16, 32, 64],\n",
    "            'units': [32, 50, 64],\n",
    "            'dropout_rate': [0.2, 0.3, 0.4],\n",
    "            'epochs': [30, 50, 70]\n",
    "        }\n",
    "        return param_grid\n",
    "    \n",
    "    def cross_validate_params(self, X: pd.DataFrame, y: pd.Series, params: dict):\n",
    "        \"\"\"\n",
    "        Cross-validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parameter set ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "        \"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            try:\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "                \n",
    "                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ parameters ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "                model = RNNRegressor(\n",
    "                    model_type=self.model_type,\n",
    "                    sequence_length=params['sequence_length'],\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    units=params['units'],\n",
    "                    dropout_rate=params['dropout_rate'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if len(y_pred) > 0:\n",
    "                    # Align predictions with test data\n",
    "                    y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                    y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                    \n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                    scores.append(rmse)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return np.mean(scores) if scores else float('inf')\n",
    "    \n",
    "    def grid_search(self, X: pd.DataFrame, y: pd.Series, max_combinations: int = 50):\n",
    "        \"\"\"\n",
    "        Grid search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤ hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "        \"\"\"\n",
    "        param_grid = self.define_param_grid()\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = list(param_grid.values())\n",
    "        all_combinations = list(product(*param_values))\n",
    "        \n",
    "        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤\n",
    "        if len(all_combinations) > max_combinations:\n",
    "            selected_combinations = random.sample(all_combinations, max_combinations)\n",
    "        else:\n",
    "            selected_combinations = all_combinations\n",
    "        \n",
    "        print(f\"\\n[Hyperparameter Tuning] Testing {len(selected_combinations)} parameter combinations for {self.model_type}...\")\n",
    "        \n",
    "        for i, combination in enumerate(selected_combinations):\n",
    "            params = dict(zip(param_names, combination))\n",
    "            \n",
    "            print(f\"\\n[{i+1}/{len(selected_combinations)}] Testing: {params}\")\n",
    "            \n",
    "            score = self.cross_validate_params(X, y, params)\n",
    "            \n",
    "            self.tuning_results.append({\n",
    "                'params': params.copy(),\n",
    "                'score': score\n",
    "            })\n",
    "            \n",
    "            if score < self.best_score:\n",
    "                self.best_score = score\n",
    "                self.best_params = params.copy()\n",
    "                print(f\"New best score: {score:.4f}\")\n",
    "        \n",
    "        print(f\"\\n[Hyperparameter Tuning] Best score: {self.best_score:.4f}\")\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return self.best_params, self.best_score\n",
    "    \n",
    "    def get_tuning_summary(self):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ tuning\n",
    "        \"\"\"\n",
    "        if not self.tuning_results:\n",
    "            return \"No tuning results available\"\n",
    "        \n",
    "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° score\n",
    "        sorted_results = sorted(self.tuning_results, key=lambda x: x['score'])\n",
    "        \n",
    "        summary = f\"\\n{'='*60}\\n\"\n",
    "        summary += f\"HYPERPARAMETER TUNING SUMMARY - {self.model_type}\\n\"\n",
    "        summary += f\"{'='*60}\\n\"\n",
    "        summary += f\"Total combinations tested: {len(self.tuning_results)}\\n\"\n",
    "        summary += f\"Best score (RMSE): {self.best_score:.4f}\\n\"\n",
    "        summary += f\"Best parameters:\\n\"\n",
    "        \n",
    "        for param, value in self.best_params.items():\n",
    "            summary += f\"  {param}: {value}\\n\"\n",
    "        \n",
    "        summary += f\"\\nTop 5 parameter combinations:\\n\"\n",
    "        summary += f\"{'-'*60}\\n\"\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:5]):\n",
    "            summary += f\"{i+1}. Score: {result['score']:.4f}\\n\"\n",
    "            for param, value in result['params'].items():\n",
    "                summary += f\"   {param}: {value}\\n\"\n",
    "            summary += \"\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "class DriftPointDetector:\n",
    "    \"\"\"\n",
    "    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏∏‡∏î‡πÄ‡∏Å‡∏¥‡∏î concept drift ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ\n",
    "    ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö pattern ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size: int = 120, threshold: float = 0.001, \n",
    "                 step_size: int = 30, min_effect_size: float = 0.3,\n",
    "                 stability_window: int = 60, confirmation_tests: int = 2): \n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.step_size = step_size\n",
    "        self.min_effect_size = min_effect_size\n",
    "        self.stability_window = stability_window\n",
    "        self.confirmation_tests = confirmation_tests\n",
    "        self.drift_points_: List[int] = []\n",
    "\n",
    "    def _calculate_effect_size(self, window1: pd.Series, window2: pd.Series) -> float:\n",
    "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö (Cohen's d)\"\"\"\n",
    "        mean1, mean2 = window1.mean(), window2.mean()\n",
    "        std1, std2 = window1.std(), window2.std()\n",
    "        \n",
    "        pooled_std = np.sqrt(((len(window1) - 1) * std1**2 + (len(window2) - 1) * std2**2) / \n",
    "                           (len(window1) + len(window2) - 2))\n",
    "        \n",
    "        if pooled_std == 0:\n",
    "            return 0\n",
    "        \n",
    "        return abs(mean1 - mean2) / pooled_std\n",
    "\n",
    "    def _test_multiple_statistics(self, window1: pd.DataFrame, window2: pd.DataFrame) -> Tuple[int, float]:\n",
    "        \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô drift\"\"\"\n",
    "        passed_tests = 0\n",
    "        min_p_value = 1.0\n",
    "        \n",
    "        for col in window1.columns:\n",
    "            col_tests = 0\n",
    "            col_p_values = []\n",
    "            \n",
    "            # Test 1: Kolmogorov-Smirnov test\n",
    "            try:\n",
    "                stat, p_value = ks_2samp(window1[col], window2[col])\n",
    "                col_p_values.append(p_value)\n",
    "                if p_value < self.threshold:\n",
    "                    col_tests += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Test 2: Mann-Whitney U test\n",
    "            try:\n",
    "                stat, p_value = mannwhitneyu(window1[col], window2[col], alternative='two-sided')\n",
    "                col_p_values.append(p_value)\n",
    "                if p_value < self.threshold:\n",
    "                    col_tests += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Test 3: Effect size check\n",
    "            effect_size = self._calculate_effect_size(window1[col], window2[col])\n",
    "            if effect_size > self.min_effect_size:\n",
    "                col_tests += 1\n",
    "            \n",
    "            if col_p_values:\n",
    "                min_p_value = min(min_p_value, min(col_p_values))\n",
    "            \n",
    "            if col_tests >= self.confirmation_tests:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        return passed_tests, min_p_value\n",
    "\n",
    "    def _check_stability_before_drift(self, X: pd.DataFrame, position: int) -> bool:\n",
    "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
    "        if position < self.stability_window + self.window_size:\n",
    "            return True\n",
    "        \n",
    "        stable_start = position - self.stability_window - self.window_size\n",
    "        stable_end = position - self.window_size\n",
    "        stable_window = X.iloc[stable_start:stable_end]\n",
    "        \n",
    "        mid_point = len(stable_window) // 2\n",
    "        stable_part1 = stable_window.iloc[:mid_point]\n",
    "        stable_part2 = stable_window.iloc[mid_point:]\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if len(stable_part1) > 0 and len(stable_part2) > 0:\n",
    "                try:\n",
    "                    stat, p_value = ks_2samp(stable_part1[col], stable_part2[col])\n",
    "                    if p_value < self.threshold * 10:\n",
    "                        return False\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _remove_pattern_drifts(self, drift_candidates: List[Tuple[int, float]]) -> List[int]:\n",
    "        \"\"\"‡∏Å‡∏£‡∏≠‡∏á‡∏à‡∏∏‡∏î drift ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô pattern\"\"\"\n",
    "        if len(drift_candidates) < 3:\n",
    "            return [pos for pos, _ in drift_candidates]\n",
    "        \n",
    "        drift_candidates.sort(key=lambda x: x[0])\n",
    "        \n",
    "        intervals = []\n",
    "        for i in range(1, len(drift_candidates)):\n",
    "            interval = drift_candidates[i][0] - drift_candidates[i-1][0]\n",
    "            intervals.append(interval)\n",
    "        \n",
    "        filtered_drifts = []\n",
    "        if len(intervals) > 1:\n",
    "            interval_std = np.std(intervals)\n",
    "            interval_mean = np.mean(intervals)\n",
    "            \n",
    "            if interval_std / interval_mean < 0.3: \n",
    "                drift_candidates.sort(key=lambda x: x[1])\n",
    "                keep_count = max(1, len(drift_candidates) // 3)\n",
    "                filtered_drifts = [pos for pos, _ in drift_candidates[:keep_count]]\n",
    "            else:\n",
    "                filtered_drifts = [pos for pos, _ in drift_candidates]\n",
    "        else:\n",
    "            filtered_drifts = [pos for pos, _ in drift_candidates]\n",
    "        \n",
    "        final_drifts = []\n",
    "        min_distance = self.window_size * 2\n",
    "        \n",
    "        for pos in sorted(filtered_drifts):\n",
    "            if not final_drifts or pos - final_drifts[-1] >= min_distance:\n",
    "                final_drifts.append(pos)\n",
    "        \n",
    "        return final_drifts\n",
    "\n",
    "    def detect(self, X: pd.DataFrame) -> List[int]:\n",
    "        self.drift_points_ = []\n",
    "        n = len(X)\n",
    "        drift_candidates = []\n",
    "        \n",
    "        for i in range(self.window_size, n - self.window_size, self.step_size):\n",
    "            if not self._check_stability_before_drift(X, i):\n",
    "                continue\n",
    "            \n",
    "            window1 = X.iloc[i - self.window_size:i]\n",
    "            window2 = X.iloc[i:i + self.window_size]\n",
    "            \n",
    "            passed_tests, min_p_value = self._test_multiple_statistics(window1, window2)\n",
    "            \n",
    "            if passed_tests >= 1:\n",
    "                drift_candidates.append((i, min_p_value))\n",
    "        \n",
    "        self.drift_points_ = self._remove_pattern_drifts(drift_candidates)\n",
    "        \n",
    "        return self.drift_points_\n",
    "\n",
    "class AdaptiveFoldGenerator:\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á train/test folds ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° drift points ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ\n",
    "    \"\"\"\n",
    "    def __init__(self, min_fold_size: int = 120, test_ratio: float = 0.2):\n",
    "        self.min_fold_size = min_fold_size\n",
    "        self.test_ratio = test_ratio\n",
    "\n",
    "    def split(self, X: pd.DataFrame, drift_points: List[int]) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        folds = []\n",
    "        points = [0] + drift_points + [len(X)]\n",
    "        \n",
    "        for i in range(len(points) - 1):\n",
    "            start, end = points[i], points[i + 1]\n",
    "            fold_length = end - start\n",
    "\n",
    "            if fold_length < self.min_fold_size:\n",
    "                continue\n",
    "\n",
    "            split = int(start + (1 - self.test_ratio) * fold_length)\n",
    "            train_idx = np.arange(start, split)\n",
    "            test_idx = np.arange(split, end)\n",
    "\n",
    "            if len(train_idx) > 100 and len(test_idx) > 50:\n",
    "                folds.append((train_idx, test_idx))\n",
    "        \n",
    "        return folds\n",
    "\n",
    "class DriftAdaptiveTimeSeriesCV:\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥ cross-validation ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ fold ‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° drift points ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡πÅ‡∏•‡∏∞ Linear Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {\n",
    "            'sequence_length': 30,\n",
    "            'units': 50,\n",
    "            'dropout_rate': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32,\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]) -> Tuple[List[float], List[float]]:\n",
    "        fold_gen = AdaptiveFoldGenerator()\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "\n",
    "        folds = fold_gen.split(X, drift_points)\n",
    "        if not folds:\n",
    "            print(\"Warning: No valid folds generated by AdaptiveFoldGenerator!\")\n",
    "            return [], []\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(folds):\n",
    "            print(f\"\\n[Adaptive Fold {i+1}] Training {self.model_type}...\")\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "            elif self.model_type == 'LINEAR':\n",
    "                model = LinearRegressionModel(**self.model_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    if len(y_pred) > 0:\n",
    "                        y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                        y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                        mae = mean_absolute_error(y_test_aligned, y_pred)\n",
    "                    else:\n",
    "                        print(f\"[Adaptive Fold {i+1}] No predictions generated (insufficient data)\")\n",
    "                        continue\n",
    "                else:\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                print(f\"[Adaptive Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                \n",
    "                metrics_rmse.append(rmse)\n",
    "                metrics_mae.append(mae)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[Adaptive Fold {i+1}] Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        return metrics_rmse, metrics_mae\n",
    "\n",
    "class BaselineTimeSeriesCV:\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥ cross-validation ‡πÅ‡∏ö‡∏ö TimeSeriesSplit ‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡πÅ‡∏•‡∏∞ Linear Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None, n_splits: int = 5):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {\n",
    "            'sequence_length': 30,\n",
    "            'units': 50,\n",
    "            'dropout_rate': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[float], List[float]]:\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "            print(f\"\\n[Baseline Fold {i+1}] Training {self.model_type}...\")\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "            elif self.model_type == 'LINEAR':\n",
    "                model = LinearRegressionModel(**self.model_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    if len(y_pred) > 0:\n",
    "                        y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                        y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                        mae = mean_absolute_error(y_test_aligned, y_pred)\n",
    "                    else:\n",
    "                        print(f\"[Baseline Fold {i+1}] No predictions generated (insufficient data)\")\n",
    "                        continue\n",
    "                else:\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                print(f\"[Baseline Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                \n",
    "                metrics_rmse.append(rmse)\n",
    "                metrics_mae.append(mae)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[Baseline Fold {i+1}] Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        return metrics_rmse, metrics_mae\n",
    "\n",
    "class EnhancedModelComparison:\n",
    "    \"\"\"\n",
    "    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á RNN, LSTM, GRU ‡πÅ‡∏•‡∏∞ Linear Regression ‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, enable_tuning: bool = True, tuning_max_combinations: int = 30):\n",
    "        self.enable_tuning = enable_tuning\n",
    "        self.tuning_max_combinations = tuning_max_combinations\n",
    "        self.models = ['RNN', 'LSTM', 'GRU', 'LINEAR']\n",
    "        self.optimized_params = {}\n",
    "        self.tuning_summaries = {}\n",
    "        \n",
    "    def tune_hyperparameters(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        ‡∏ó‡∏≥ hyperparameter tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "            print(f\"\\n[TUNING] Starting hyperparameter tuning for {model_type}...\")\n",
    "            \n",
    "            tuner = HyperparameterTuner(model_type=model_type, n_splits=3)\n",
    "            best_params, best_score = tuner.grid_search(X, y, max_combinations=self.tuning_max_combinations)\n",
    "            \n",
    "            self.optimized_params[model_type] = best_params\n",
    "            self.tuning_summaries[model_type] = tuner.get_tuning_summary()\n",
    "            \n",
    "            print(f\"[TUNING] Completed tuning for {model_type}\")\n",
    "        \n",
    "        # Linear Regression ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á tune\n",
    "        self.optimized_params['LINEAR'] = {'fit_intercept': True}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HYPERPARAMETER TUNING COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    def compare_models(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]):\n",
    "        \"\"\"\n",
    "        ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢ adaptive CV ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ optimized parameters\n",
    "        \"\"\"\n",
    "        # ‡∏ó‡∏≥ hyperparameter tuning ‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "        if self.enable_tuning:\n",
    "            self.tune_hyperparameters(X, y)\n",
    "        else:\n",
    "            # ‡πÉ‡∏ä‡πâ default parameters\n",
    "            default_rnn_params = {\n",
    "                'sequence_length': 30,\n",
    "                'units': 50,\n",
    "                'dropout_rate': 0.3,\n",
    "                'learning_rate': 0.001,\n",
    "                'epochs': 50,\n",
    "                'batch_size': 32,\n",
    "                'verbose': 0\n",
    "            }\n",
    "            for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.optimized_params[model_type] = default_rnn_params\n",
    "            self.optimized_params['LINEAR'] = {'fit_intercept': True}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Testing {model_type} Model with Optimized Parameters\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            params = self.optimized_params[model_type]\n",
    "            print(f\"Using parameters: {params}\")\n",
    "            \n",
    "            # Adaptive CV\n",
    "            drift_cv = DriftAdaptiveTimeSeriesCV(model_type, params)\n",
    "            drift_rmse, drift_mae = drift_cv.run(X, y, drift_points)\n",
    "            \n",
    "            # Baseline CV\n",
    "            baseline_cv = BaselineTimeSeriesCV(model_type, params, n_splits=5)\n",
    "            base_rmse, base_mae = baseline_cv.run(X, y)\n",
    "            \n",
    "            results[model_type] = {\n",
    "                'adaptive_rmse': drift_rmse,\n",
    "                'adaptive_mae': drift_mae,\n",
    "                'baseline_rmse': base_rmse,\n",
    "                'baseline_mae': base_mae,\n",
    "                'optimized_params': params\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: dict):\n",
    "        \"\"\"\n",
    "        ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á optimized parameters\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL COMPARISON SUMMARY WITH OPTIMIZED PARAMETERS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£ tuning ‡∏Å‡πà‡∏≠‡∏ô\n",
    "        if self.enable_tuning:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                if model_type in self.tuning_summaries:\n",
    "                    print(self.tuning_summaries[model_type])\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        performance_summary = []\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                print(f\"\\n{model_type} Results:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # ‡πÅ‡∏™‡∏î‡∏á optimized parameters\n",
    "                print(f\"Optimized Parameters: {results[model_type]['optimized_params']}\")\n",
    "                \n",
    "                # Adaptive results\n",
    "                if results[model_type]['adaptive_rmse'] and results[model_type]['adaptive_mae']:\n",
    "                    avg_rmse = np.mean(results[model_type]['adaptive_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['adaptive_mae'])\n",
    "                    print(f\"Adaptive CV - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}\")\n",
    "                    \n",
    "                    performance_summary.append({\n",
    "                        'model': model_type,\n",
    "                        'adaptive_rmse': avg_rmse,\n",
    "                        'adaptive_mae': avg_mae,\n",
    "                        'params': results[model_type]['optimized_params']\n",
    "                    })\n",
    "                else:\n",
    "                    print(\"Adaptive CV - No valid results\")\n",
    "                \n",
    "                # Baseline results\n",
    "                if results[model_type]['baseline_rmse'] and results[model_type]['baseline_mae']:\n",
    "                    avg_rmse = np.mean(results[model_type]['baseline_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['baseline_mae'])\n",
    "                    print(f\"Baseline CV - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}\")\n",
    "                else:\n",
    "                    print(\"Baseline CV - No valid results\")\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á ranking ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "        if performance_summary:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"MODEL RANKING (Based on Adaptive CV RMSE)\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            sorted_models = sorted(performance_summary, key=lambda x: x['adaptive_rmse'])\n",
    "            \n",
    "            for i, model_info in enumerate(sorted_models, 1):\n",
    "                print(f\"\\n{i}. {model_info['model']}\")\n",
    "                print(f\"   RMSE: {model_info['adaptive_rmse']:.4f}\")\n",
    "                print(f\"   MAE: {model_info['adaptive_mae']:.4f}\")\n",
    "                print(f\"   Best Parameters: {model_info['params']}\")\n",
    "        \n",
    "        # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "        best_model_info = self._find_best_model_with_params(results)\n",
    "        if best_model_info:\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"üèÜ BEST MODEL: {best_model_info['model']}\")\n",
    "            print(f\"   RMSE: {best_model_info['score']:.4f}\")\n",
    "            print(f\"   Optimal Parameters: {best_model_info['params']}\")\n",
    "            print(\"=\"*60)\n",
    "    \n",
    "    def _find_best_model_with_params(self, results: dict):\n",
    "        \"\"\"\n",
    "        ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå\n",
    "        \"\"\"\n",
    "        best_model_info = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                # ‡πÉ‡∏ä‡πâ adaptive RMSE ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
    "                if results[model_type]['adaptive_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['adaptive_rmse'])\n",
    "                    if avg_rmse < best_score:\n",
    "                        best_score = avg_rmse\n",
    "                        best_model_info = {\n",
    "                            'model': model_type,\n",
    "                            'score': avg_rmse,\n",
    "                            'params': results[model_type]['optimized_params']\n",
    "                        }\n",
    "                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ adaptive results ‡πÉ‡∏ä‡πâ baseline\n",
    "                elif results[model_type]['baseline_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['baseline_rmse'])\n",
    "                    if avg_rmse < best_score:\n",
    "                        best_score = avg_rmse\n",
    "                        best_model_info = {\n",
    "                            'model': model_type,\n",
    "                            'score': avg_rmse,\n",
    "                            'params': results[model_type]['optimized_params']\n",
    "                        }\n",
    "        \n",
    "        return best_model_info\n",
    "    \n",
    "    def get_best_params_for_model(self, model_type: str):\n",
    "        \"\"\"\n",
    "        ‡∏î‡∏∂‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "        \"\"\"\n",
    "        if model_type.upper() in self.optimized_params:\n",
    "            return self.optimized_params[model_type.upper()]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Detect drift points\n",
    "    print(\"Starting Drift Detection...\")\n",
    "    detector = DriftPointDetector(\n",
    "        window_size=120,\n",
    "        threshold=0.001,  \n",
    "        step_size=30,\n",
    "        min_effect_size=0.3,\n",
    "        stability_window=60,\n",
    "        confirmation_tests=2\n",
    "    )\n",
    "    drift_points = detector.detect(X)\n",
    "    print(f\"Detected {len(drift_points)} drift points\")\n",
    "    \n",
    "    # 2) ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "    print(\"\\nStarting Enhanced Model Comparison with Hyperparameter Tuning...\")\n",
    "    \n",
    "    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô hyperparameter tuning (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)\n",
    "    enhanced_comparator = EnhancedModelComparison(\n",
    "        enable_tuning=True, \n",
    "        tuning_max_combinations=25  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤\n",
    "    )\n",
    "    \n",
    "    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ default parameters (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)\n",
    "    # enhanced_comparator = EnhancedModelComparison(enable_tuning=False)\n",
    "    \n",
    "    results = enhanced_comparator.compare_models(X, y, drift_points)\n",
    "    enhanced_comparator.print_summary(results)\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST PARAMETERS FOR EACH MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_type in ['RNN', 'LSTM', 'GRU', 'LINEAR']:\n",
    "        best_params = enhanced_comparator.get_best_params_for_model(model_type)\n",
    "        if best_params:\n",
    "            print(f\"\\n{model_type} Best Parameters:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á drift point index ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö ‡∏ß‡∏±‡∏ô/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô/‡∏õ‡∏µ\n",
    "    drift_dates_formatted = df.iloc[drift_points]['Date'].dt.strftime('%d/%m/%Y').tolist()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DRIFT DETECTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Detected Drift Points: {len(drift_points)}\")\n",
    "    print(\"Drift Dates:\")\n",
    "    for i, date in enumerate(drift_dates_formatted, 1):\n",
    "        print(f\"  {i}. {date}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c6e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drift Detection...\n",
      "Detected 8 drift points\n",
      "\n",
      "Starting Enhanced Model Comparison with Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "STARTING HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for RNN...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for RNN...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 26.9411\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 23.8188\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 22.9561\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 22.9561\n",
      "Best parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "[TUNING] Completed tuning for RNN\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for LSTM...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for LSTM...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "New best score: 22.1510\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 22.1510\n",
      "Best parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "[TUNING] Completed tuning for LSTM\n",
      "\n",
      "[TUNING] Starting hyperparameter tuning for GRU...\n",
      "\n",
      "[Hyperparameter Tuning] Testing 25 parameter combinations for GRU...\n",
      "\n",
      "[1/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "New best score: 24.1865\n",
      "\n",
      "[2/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "New best score: 23.8478\n",
      "\n",
      "[3/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.4, 'epochs': 70}\n",
      "\n",
      "[4/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "New best score: 23.0286\n",
      "\n",
      "[5/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[6/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[7/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 32, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 22.8495\n",
      "\n",
      "[8/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[9/25] Testing: {'sequence_length': 30, 'learning_rate': 0.01, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[10/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[11/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.3, 'epochs': 50}\n",
      "\n",
      "[12/25] Testing: {'sequence_length': 45, 'learning_rate': 0.001, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.3, 'epochs': 70}\n",
      "New best score: 22.5575\n",
      "\n",
      "[13/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "New best score: 21.2010\n",
      "\n",
      "[14/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[15/25] Testing: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[16/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[17/25] Testing: {'sequence_length': 45, 'learning_rate': 0.01, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[18/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[19/25] Testing: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "New best score: 21.1638\n",
      "\n",
      "[20/25] Testing: {'sequence_length': 20, 'learning_rate': 0.05, 'batch_size': 64, 'units': 50, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[21/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 50, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[22/25] Testing: {'sequence_length': 30, 'learning_rate': 0.05, 'batch_size': 32, 'units': 50, 'dropout_rate': 0.3, 'epochs': 30}\n",
      "\n",
      "[23/25] Testing: {'sequence_length': 20, 'learning_rate': 0.01, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.2, 'epochs': 50}\n",
      "\n",
      "[24/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 16, 'units': 32, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[25/25] Testing: {'sequence_length': 45, 'learning_rate': 0.05, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 30}\n",
      "\n",
      "[Hyperparameter Tuning] Best score: 21.1638\n",
      "Best parameters: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "[TUNING] Completed tuning for GRU\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETED\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Testing RNN Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "[Adaptive Fold 1] Training RNN...\n",
      "[Adaptive Fold 1] RMSE=0.423, MAE=0.413\n",
      "\n",
      "[Adaptive Fold 2] Training RNN...\n",
      "[Adaptive Fold 2] RMSE=1.193, MAE=1.019\n",
      "\n",
      "[Adaptive Fold 3] Training RNN...\n",
      "[Adaptive Fold 3] RMSE=7.235, MAE=7.140\n",
      "\n",
      "[Adaptive Fold 4] Training RNN...\n",
      "[Adaptive Fold 4] RMSE=25.075, MAE=24.274\n",
      "\n",
      "[Baseline Fold 1] Training RNN...\n",
      "[Baseline Fold 1] RMSE=2.990, MAE=2.634\n",
      "\n",
      "[Baseline Fold 2] Training RNN...\n",
      "[Baseline Fold 2] RMSE=1.355, MAE=1.169\n",
      "\n",
      "[Baseline Fold 3] Training RNN...\n",
      "[Baseline Fold 3] RMSE=8.054, MAE=6.927\n",
      "\n",
      "[Baseline Fold 4] Training RNN...\n",
      "[Baseline Fold 4] RMSE=5.682, MAE=4.357\n",
      "\n",
      "[Baseline Fold 5] Training RNN...\n",
      "[Baseline Fold 5] RMSE=68.192, MAE=57.286\n",
      "\n",
      "==================================================\n",
      "Testing LSTM Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "[Adaptive Fold 1] Training LSTM...\n",
      "[Adaptive Fold 1] RMSE=0.558, MAE=0.537\n",
      "\n",
      "[Adaptive Fold 2] Training LSTM...\n",
      "[Adaptive Fold 2] RMSE=1.037, MAE=0.886\n",
      "\n",
      "[Adaptive Fold 3] Training LSTM...\n",
      "[Adaptive Fold 3] RMSE=7.885, MAE=7.817\n",
      "\n",
      "[Adaptive Fold 4] Training LSTM...\n",
      "[Adaptive Fold 4] RMSE=24.072, MAE=23.450\n",
      "\n",
      "[Baseline Fold 1] Training LSTM...\n",
      "[Baseline Fold 1] RMSE=2.825, MAE=2.459\n",
      "\n",
      "[Baseline Fold 2] Training LSTM...\n",
      "[Baseline Fold 2] RMSE=0.952, MAE=0.836\n",
      "\n",
      "[Baseline Fold 3] Training LSTM...\n",
      "[Baseline Fold 3] RMSE=7.078, MAE=6.033\n",
      "\n",
      "[Baseline Fold 4] Training LSTM...\n",
      "[Baseline Fold 4] RMSE=4.440, MAE=3.235\n",
      "\n",
      "[Baseline Fold 5] Training LSTM...\n",
      "[Baseline Fold 5] RMSE=65.999, MAE=54.843\n",
      "\n",
      "==================================================\n",
      "Testing GRU Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "[Adaptive Fold 1] Training GRU...\n",
      "[Adaptive Fold 1] RMSE=0.375, MAE=0.373\n",
      "\n",
      "[Adaptive Fold 2] Training GRU...\n",
      "[Adaptive Fold 2] RMSE=0.814, MAE=0.763\n",
      "\n",
      "[Adaptive Fold 3] Training GRU...\n",
      "[Adaptive Fold 3] RMSE=7.963, MAE=7.909\n",
      "\n",
      "[Adaptive Fold 4] Training GRU...\n",
      "[Adaptive Fold 4] RMSE=13.351, MAE=12.343\n",
      "\n",
      "[Baseline Fold 1] Training GRU...\n",
      "[Baseline Fold 1] RMSE=2.688, MAE=2.338\n",
      "\n",
      "[Baseline Fold 2] Training GRU...\n",
      "[Baseline Fold 2] RMSE=0.987, MAE=0.891\n",
      "\n",
      "[Baseline Fold 3] Training GRU...\n",
      "[Baseline Fold 3] RMSE=6.980, MAE=6.025\n",
      "\n",
      "[Baseline Fold 4] Training GRU...\n",
      "[Baseline Fold 4] RMSE=5.101, MAE=3.763\n",
      "\n",
      "[Baseline Fold 5] Training GRU...\n",
      "[Baseline Fold 5] RMSE=66.000, MAE=55.708\n",
      "\n",
      "==================================================\n",
      "Testing LINEAR Model with Optimized Parameters\n",
      "==================================================\n",
      "Using parameters: {'fit_intercept': True}\n",
      "\n",
      "[Adaptive Fold 1] Training LINEAR...\n",
      "[Adaptive Fold 1] RMSE=0.557, MAE=0.535\n",
      "\n",
      "[Adaptive Fold 2] Training LINEAR...\n",
      "[Adaptive Fold 2] RMSE=1.268, MAE=1.167\n",
      "\n",
      "[Adaptive Fold 3] Training LINEAR...\n",
      "[Adaptive Fold 3] RMSE=5.417, MAE=5.018\n",
      "\n",
      "[Adaptive Fold 4] Training LINEAR...\n",
      "[Adaptive Fold 4] RMSE=24.792, MAE=22.538\n",
      "\n",
      "[Baseline Fold 1] Training LINEAR...\n",
      "[Baseline Fold 1] RMSE=1.762, MAE=1.374\n",
      "\n",
      "[Baseline Fold 2] Training LINEAR...\n",
      "[Baseline Fold 2] RMSE=2.197, MAE=1.682\n",
      "\n",
      "[Baseline Fold 3] Training LINEAR...\n",
      "[Baseline Fold 3] RMSE=4.775, MAE=3.956\n",
      "\n",
      "[Baseline Fold 4] Training LINEAR...\n",
      "[Baseline Fold 4] RMSE=8.338, MAE=6.033\n",
      "\n",
      "[Baseline Fold 5] Training LINEAR...\n",
      "[Baseline Fold 5] RMSE=32.602, MAE=26.697\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY WITH OPTIMIZED PARAMETERS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING RESULTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - RNN\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 22.9561\n",
      "Best parameters:\n",
      "  sequence_length: 20\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 16\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 22.9561\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 16\n",
      "   units: 64\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "2. Score: 22.9819\n",
      "   sequence_length: 30\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 70\n",
      "\n",
      "3. Score: 23.1498\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 32\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 30\n",
      "\n",
      "4. Score: 23.3088\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 32\n",
      "   dropout_rate: 0.3\n",
      "   epochs: 70\n",
      "\n",
      "5. Score: 23.3907\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 32\n",
      "   dropout_rate: 0.3\n",
      "   epochs: 30\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - LSTM\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 22.1510\n",
      "Best parameters:\n",
      "  sequence_length: 20\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 64\n",
      "  units: 64\n",
      "  dropout_rate: 0.2\n",
      "  epochs: 70\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 22.1510\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 64\n",
      "   units: 64\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 70\n",
      "\n",
      "2. Score: 22.4776\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 64\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 30\n",
      "\n",
      "3. Score: 22.5024\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 50\n",
      "\n",
      "4. Score: 22.5230\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.01\n",
      "   batch_size: 64\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "5. Score: 22.7234\n",
      "   sequence_length: 30\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 16\n",
      "   units: 50\n",
      "   dropout_rate: 0.3\n",
      "   epochs: 30\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - GRU\n",
      "============================================================\n",
      "Total combinations tested: 25\n",
      "Best score (RMSE): 21.1638\n",
      "Best parameters:\n",
      "  sequence_length: 30\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 32\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 30\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "------------------------------------------------------------\n",
      "1. Score: 21.1638\n",
      "   sequence_length: 30\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 64\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 30\n",
      "\n",
      "2. Score: 21.2010\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 32\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 30\n",
      "\n",
      "3. Score: 21.7555\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 64\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "4. Score: 22.4531\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.05\n",
      "   batch_size: 64\n",
      "   units: 50\n",
      "   dropout_rate: 0.4\n",
      "   epochs: 50\n",
      "\n",
      "5. Score: 22.5325\n",
      "   sequence_length: 20\n",
      "   learning_rate: 0.01\n",
      "   batch_size: 16\n",
      "   units: 50\n",
      "   dropout_rate: 0.2\n",
      "   epochs: 70\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "RNN Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "Adaptive CV - Avg RMSE: 8.4812, Avg MAE: 8.2114\n",
      "Baseline CV - Avg RMSE: 17.2545, Avg MAE: 14.4747\n",
      "\n",
      "LSTM Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "Adaptive CV - Avg RMSE: 8.3881, Avg MAE: 8.1725\n",
      "Baseline CV - Avg RMSE: 16.2588, Avg MAE: 13.4813\n",
      "\n",
      "GRU Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "Adaptive CV - Avg RMSE: 5.6260, Avg MAE: 5.3468\n",
      "Baseline CV - Avg RMSE: 16.3513, Avg MAE: 13.7448\n",
      "\n",
      "LINEAR Results:\n",
      "----------------------------------------\n",
      "Optimized Parameters: {'fit_intercept': True}\n",
      "Adaptive CV - Avg RMSE: 8.0087, Avg MAE: 7.3146\n",
      "Baseline CV - Avg RMSE: 9.9348, Avg MAE: 7.9484\n",
      "\n",
      "============================================================\n",
      "MODEL RANKING (Based on Adaptive CV RMSE)\n",
      "============================================================\n",
      "\n",
      "1. GRU\n",
      "   RMSE: 5.6260\n",
      "   MAE: 5.3468\n",
      "   Best Parameters: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "\n",
      "2. LINEAR\n",
      "   RMSE: 8.0087\n",
      "   MAE: 7.3146\n",
      "   Best Parameters: {'fit_intercept': True}\n",
      "\n",
      "3. LSTM\n",
      "   RMSE: 8.3881\n",
      "   MAE: 8.1725\n",
      "   Best Parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 64, 'units': 64, 'dropout_rate': 0.2, 'epochs': 70}\n",
      "\n",
      "4. RNN\n",
      "   RMSE: 8.4812\n",
      "   MAE: 8.2114\n",
      "   Best Parameters: {'sequence_length': 20, 'learning_rate': 0.001, 'batch_size': 16, 'units': 64, 'dropout_rate': 0.4, 'epochs': 50}\n",
      "\n",
      "============================================================\n",
      "üèÜ BEST MODEL: GRU\n",
      "   RMSE: 5.6260\n",
      "   Optimal Parameters: {'sequence_length': 30, 'learning_rate': 0.001, 'batch_size': 32, 'units': 64, 'dropout_rate': 0.4, 'epochs': 30}\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "BEST PARAMETERS FOR EACH MODEL\n",
      "================================================================================\n",
      "\n",
      "RNN Best Parameters:\n",
      "  sequence_length: 20\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 16\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 50\n",
      "\n",
      "LSTM Best Parameters:\n",
      "  sequence_length: 20\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 64\n",
      "  units: 64\n",
      "  dropout_rate: 0.2\n",
      "  epochs: 70\n",
      "\n",
      "GRU Best Parameters:\n",
      "  sequence_length: 30\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 32\n",
      "  units: 64\n",
      "  dropout_rate: 0.4\n",
      "  epochs: 30\n",
      "\n",
      "LINEAR Best Parameters:\n",
      "  fit_intercept: True\n",
      "\n",
      "================================================================================\n",
      "DRIFT DETECTION RESULTS\n",
      "================================================================================\n",
      "Detected Drift Points: 8\n",
      "Drift Dates:\n",
      "  1. 09/07/2015\n",
      "  2. 15/09/2016\n",
      "  3. 29/08/2017\n",
      "  4. 14/06/2019\n",
      "  5. 28/05/2020\n",
      "  6. 29/10/2021\n",
      "  7. 13/10/2022\n",
      "  8. 28/09/2023\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ TensorFlow ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á!\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "\n",
    "# ===== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞ reproducibility =====\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ‡∏õ‡∏¥‡∏î multi-threading ‡∏Ç‡∏≠‡∏á TensorFlow\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocess data\n",
    "df = pd.read_csv(\"nvidia_10yr_data.csv\", parse_dates=[\"Date\"])\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "# Feature engineering\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df['Volatility'] = df['Close'].rolling(10).std()\n",
    "df['Price_Diff'] = df['High'] - df['Low']\n",
    "df['Volume_Log'] = np.log1p(df['Volume'])\n",
    "\n",
    "# Drop NaN ‡∏´‡∏•‡∏±‡∏á rolling\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df[['Return', 'Volatility', 'Price_Diff', 'Volume_Log']]\n",
    "y = df['Close']\n",
    "\n",
    "class SequenceGenerator:\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á sequence data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN-based models\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length: int = 30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "    def create_sequences(self, X: pd.DataFrame, y: pd.Series, fit_scalers: bool = True):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡πâ‡∏≤‡∏á sequence data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN-based models\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        if fit_scalers:\n",
    "            X_scaled = self.scaler_X.fit_transform(X)\n",
    "            y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            X_scaled = self.scaler_X.transform(X)\n",
    "            y_scaled = self.scaler_y.transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create sequences\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(self.sequence_length, len(X_scaled)):\n",
    "            X_seq.append(X_scaled[i-self.sequence_length:i])\n",
    "            y_seq.append(y_scaled[i])\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    def inverse_transform_y(self, y_scaled):\n",
    "        \"\"\"\n",
    "        ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ y ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏Å‡∏•‡πÄ‡∏î‡∏¥‡∏°\n",
    "        \"\"\"\n",
    "        return self.scaler_y.inverse_transform(y_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "class RNNRegressor:\n",
    "    \"\"\"\n",
    "    Universal RNN Regressor ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö RNN, LSTM, ‡πÅ‡∏•‡∏∞ GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', sequence_length: int = 30, \n",
    "                 units: int = 50, dropout_rate: float = 0.2, \n",
    "                 learning_rate: float = 0.01, epochs: int = 100, \n",
    "                 batch_size: int = 32, verbose: int = 0):\n",
    "        \n",
    "        self.model_type = model_type.upper()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.seq_generator = SequenceGenerator(sequence_length)\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model_type ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        if self.model_type not in ['RNN', 'LSTM', 'GRU']:\n",
    "            raise ValueError(\"model_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "    def _get_layer_type(self):\n",
    "        \"\"\"\n",
    "        ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å layer type ‡∏ï‡∏≤‡∏° model_type\n",
    "        \"\"\"\n",
    "        if self.model_type == 'RNN':\n",
    "            return SimpleRNN\n",
    "        elif self.model_type == 'LSTM':\n",
    "            return LSTM\n",
    "        elif self.model_type == 'GRU':\n",
    "            return GRU\n",
    "        \n",
    "    def _build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• RNN ‡∏ï‡∏≤‡∏° model_type\n",
    "        \"\"\"\n",
    "        LayerType = self._get_layer_type()\n",
    "        \n",
    "        model = Sequential([\n",
    "            LayerType(self.units, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(self.dropout_rate),\n",
    "            LayerType(self.units // 2, return_sequences=False),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Train RNN model\n",
    "        \"\"\"\n",
    "        # Create sequences\n",
    "        X_seq, y_seq = self.seq_generator.create_sequences(X, y, fit_scalers=True)\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"Not enough data to create sequences\")\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_model((X_seq.shape[1], X_seq.shape[2]))\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(\n",
    "            X_seq, y_seq,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Create sequences (don't fit scalers)\n",
    "        X_seq, _ = self.seq_generator.create_sequences(\n",
    "            X, pd.Series([0] * len(X)), fit_scalers=False\n",
    "        )\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            # Return predictions for available data points\n",
    "            return np.array([])\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_scaled = self.model.predict(X_seq, verbose=0)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_pred = self.seq_generator.inverse_transform_y(y_pred_scaled)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "class LinearRegressionModel:\n",
    "    \"\"\"\n",
    "    Linear Regression model with standardization\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept: bool = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self.model = LinearRegression(fit_intercept=fit_intercept)\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Train Linear Regression model\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler_X.fit_transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_scaled, y_scaled)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_scaled = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', n_splits: int = 3):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.n_splits = n_splits\n",
    "        self.best_params = {}\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "        \n",
    "    def define_param_grid(self):\n",
    "        \"\"\"\n",
    "        ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ tuning\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'sequence_length': [20, 30, 45],\n",
    "            'learning_rate': [0.001, 0.01, 0.05],\n",
    "            'batch_size': [16, 32, 64],\n",
    "            'units': [32, 50, 64],\n",
    "            'dropout_rate': [0.2, 0.3, 0.4],\n",
    "            'epochs': [30, 50, 70]\n",
    "        }\n",
    "        return param_grid\n",
    "    \n",
    "    def cross_validate_params(self, X: pd.DataFrame, y: pd.Series, params: dict):\n",
    "        \"\"\"\n",
    "        Cross-validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parameter set ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "        \"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            try:\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "                \n",
    "                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ parameters ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "                model = RNNRegressor(\n",
    "                    model_type=self.model_type,\n",
    "                    sequence_length=params['sequence_length'],\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    units=params['units'],\n",
    "                    dropout_rate=params['dropout_rate'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if len(y_pred) > 0:\n",
    "                    # Align predictions with test data\n",
    "                    y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                    y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                    \n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                    scores.append(rmse)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return np.mean(scores) if scores else float('inf')\n",
    "    \n",
    "    def grid_search(self, X: pd.DataFrame, y: pd.Series, max_combinations: int = 50):\n",
    "        \"\"\"\n",
    "        Grid search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤ hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "        \"\"\"\n",
    "        param_grid = self.define_param_grid()\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = list(param_grid.values())\n",
    "        all_combinations = list(product(*param_values))\n",
    "        \n",
    "        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤\n",
    "        if len(all_combinations) > max_combinations:\n",
    "            selected_combinations = random.sample(all_combinations, max_combinations)\n",
    "        else:\n",
    "            selected_combinations = all_combinations\n",
    "        \n",
    "        print(f\"\\n[Hyperparameter Tuning] Testing {len(selected_combinations)} parameter combinations for {self.model_type}...\")\n",
    "        \n",
    "        for i, combination in enumerate(selected_combinations):\n",
    "            params = dict(zip(param_names, combination))\n",
    "            \n",
    "            print(f\"\\n[{i+1}/{len(selected_combinations)}] Testing: {params}\")\n",
    "            \n",
    "            score = self.cross_validate_params(X, y, params)\n",
    "            \n",
    "            self.tuning_results.append({\n",
    "                'params': params.copy(),\n",
    "                'score': score\n",
    "            })\n",
    "            \n",
    "            if score < self.best_score:\n",
    "                self.best_score = score\n",
    "                self.best_params = params.copy()\n",
    "                print(f\"New best score: {score:.4f}\")\n",
    "        \n",
    "        print(f\"\\n[Hyperparameter Tuning] Best score: {self.best_score:.4f}\")\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return self.best_params, self.best_score\n",
    "    \n",
    "    def get_tuning_summary(self):\n",
    "        \"\"\"\n",
    "        ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ tuning\n",
    "        \"\"\"\n",
    "        if not self.tuning_results:\n",
    "            return \"No tuning results available\"\n",
    "        \n",
    "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° score\n",
    "        sorted_results = sorted(self.tuning_results, key=lambda x: x['score'])\n",
    "        \n",
    "        summary = f\"\\n{'='*60}\\n\"\n",
    "        summary += f\"HYPERPARAMETER TUNING SUMMARY - {self.model_type}\\n\"\n",
    "        summary += f\"{'='*60}\\n\"\n",
    "        summary += f\"Total combinations tested: {len(self.tuning_results)}\\n\"\n",
    "        summary += f\"Best score (RMSE): {self.best_score:.4f}\\n\"\n",
    "        summary += f\"Best parameters:\\n\"\n",
    "        \n",
    "        for param, value in self.best_params.items():\n",
    "            summary += f\"  {param}: {value}\\n\"\n",
    "        \n",
    "        summary += f\"\\nTop 5 parameter combinations:\\n\"\n",
    "        summary += f\"{'-'*60}\\n\"\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:5]):\n",
    "            summary += f\"{i+1}. Score: {result['score']:.4f}\\n\"\n",
    "            for param, value in result['params'].items():\n",
    "                summary += f\"   {param}: {value}\\n\"\n",
    "            summary += \"\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "class DriftPointDetector:\n",
    "    \"\"\"\n",
    "    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏∏‡∏î‡πÄ‡∏Å‡∏¥‡∏î concept drift ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ\n",
    "    ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö pattern ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size: int = 120, threshold: float = 0.001, \n",
    "                 step_size: int = 30, min_effect_size: float = 0.3,\n",
    "                 stability_window: int = 60, confirmation_tests: int = 2): \n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.step_size = step_size\n",
    "        self.min_effect_size = min_effect_size\n",
    "        self.stability_window = stability_window\n",
    "        self.confirmation_tests = confirmation_tests\n",
    "        self.drift_points_: List[int] = []\n",
    "\n",
    "    def _calculate_effect_size(self, window1: pd.Series, window2: pd.Series) -> float:\n",
    "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö (Cohen's d)\"\"\"\n",
    "        mean1, mean2 = window1.mean(), window2.mean()\n",
    "        std1, std2 = window1.std(), window2.std()\n",
    "        \n",
    "        pooled_std = np.sqrt(((len(window1) - 1) * std1**2 + (len(window2) - 1) * std2**2) / \n",
    "                           (len(window1) + len(window2) - 2))\n",
    "        \n",
    "        if pooled_std == 0:\n",
    "            return 0\n",
    "        \n",
    "        return abs(mean1 - mean2) / pooled_std\n",
    "\n",
    "    def _test_multiple_statistics(self, window1: pd.DataFrame, window2: pd.DataFrame) -> Tuple[int, float]:\n",
    "        \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô drift\"\"\"\n",
    "        passed_tests = 0\n",
    "        min_p_value = 1.0\n",
    "        \n",
    "        for col in window1.columns:\n",
    "            col_tests = 0\n",
    "            col_p_values = []\n",
    "            \n",
    "            # Test 1: Kolmogorov-Smirnov test\n",
    "            try:\n",
    "                stat, p_value = ks_2samp(window1[col], window2[col])\n",
    "                col_p_values.append(p_value)\n",
    "                if p_value < self.threshold:\n",
    "                    col_tests += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Test 2: Mann-Whitney U test\n",
    "            try:\n",
    "                stat, p_value = mannwhitneyu(window1[col], window2[col], alternative='two-sided')\n",
    "                col_p_values.append(p_value)\n",
    "                if p_value < self.threshold:\n",
    "                    col_tests += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Test 3: Effect size check\n",
    "            effect_size = self._calculate_effect_size(window1[col], window2[col])\n",
    "            if effect_size > self.min_effect_size:\n",
    "                col_tests += 1\n",
    "            \n",
    "            if col_p_values:\n",
    "                min_p_value = min(min_p_value, min(col_p_values))\n",
    "            \n",
    "            if col_tests >= self.confirmation_tests:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        return passed_tests, min_p_value\n",
    "\n",
    "    def _check_stability_before_drift(self, X: pd.DataFrame, position: int) -> bool:\n",
    "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
    "        if position < self.stability_window + self.window_size:\n",
    "            return True\n",
    "        \n",
    "        stable_start = position - self.stability_window - self.window_size\n",
    "        stable_end = position - self.window_size\n",
    "        stable_window = X.iloc[stable_start:stable_end]\n",
    "        \n",
    "        mid_point = len(stable_window) // 2\n",
    "        stable_part1 = stable_window.iloc[:mid_point]\n",
    "        stable_part2 = stable_window.iloc[mid_point:]\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if len(stable_part1) > 0 and len(stable_part2) > 0:\n",
    "                try:\n",
    "                    stat, p_value = ks_2samp(stable_part1[col], stable_part2[col])\n",
    "                    if p_value < self.threshold * 10:\n",
    "                        return False\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _remove_pattern_drifts(self, drift_candidates: List[Tuple[int, float]]) -> List[int]:\n",
    "        \"\"\"‡∏Å‡∏£‡∏≠‡∏á‡∏à‡∏∏‡∏î drift ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô pattern\"\"\"\n",
    "        if len(drift_candidates) < 3:\n",
    "            return [pos for pos, _ in drift_candidates]\n",
    "        \n",
    "        drift_candidates.sort(key=lambda x: x[0])\n",
    "        \n",
    "        intervals = []\n",
    "        for i in range(1, len(drift_candidates)):\n",
    "            interval = drift_candidates[i][0] - drift_candidates[i-1][0]\n",
    "            intervals.append(interval)\n",
    "        \n",
    "        filtered_drifts = []\n",
    "        if len(intervals) > 1:\n",
    "            interval_std = np.std(intervals)\n",
    "            interval_mean = np.mean(intervals)\n",
    "            \n",
    "            if interval_std / interval_mean < 0.3: \n",
    "                drift_candidates.sort(key=lambda x: x[1])\n",
    "                keep_count = max(1, len(drift_candidates) // 3)\n",
    "                filtered_drifts = [pos for pos, _ in drift_candidates[:keep_count]]\n",
    "            else:\n",
    "                filtered_drifts = [pos for pos, _ in drift_candidates]\n",
    "        else:\n",
    "            filtered_drifts = [pos for pos, _ in drift_candidates]\n",
    "        \n",
    "        final_drifts = []\n",
    "        min_distance = self.window_size * 2\n",
    "        \n",
    "        for pos in sorted(filtered_drifts):\n",
    "            if not final_drifts or pos - final_drifts[-1] >= min_distance:\n",
    "                final_drifts.append(pos)\n",
    "        \n",
    "        return final_drifts\n",
    "\n",
    "    def detect(self, X: pd.DataFrame) -> List[int]:\n",
    "        self.drift_points_ = []\n",
    "        n = len(X)\n",
    "        drift_candidates = []\n",
    "        \n",
    "        for i in range(self.window_size, n - self.window_size, self.step_size):\n",
    "            if not self._check_stability_before_drift(X, i):\n",
    "                continue\n",
    "            \n",
    "            window1 = X.iloc[i - self.window_size:i]\n",
    "            window2 = X.iloc[i:i + self.window_size]\n",
    "            \n",
    "            passed_tests, min_p_value = self._test_multiple_statistics(window1, window2)\n",
    "            \n",
    "            if passed_tests >= 1:\n",
    "                drift_candidates.append((i, min_p_value))\n",
    "        \n",
    "        self.drift_points_ = self._remove_pattern_drifts(drift_candidates)\n",
    "        \n",
    "        return self.drift_points_\n",
    "\n",
    "class AdaptiveFoldGenerator:\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á train/test folds ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° drift points ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ\n",
    "    \"\"\"\n",
    "    def __init__(self, min_fold_size: int = 120, test_ratio: float = 0.2):\n",
    "        self.min_fold_size = min_fold_size\n",
    "        self.test_ratio = test_ratio\n",
    "\n",
    "    def split(self, X: pd.DataFrame, drift_points: List[int]) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        folds = []\n",
    "        points = [0] + drift_points + [len(X)]\n",
    "        \n",
    "        for i in range(len(points) - 1):\n",
    "            start, end = points[i], points[i + 1]\n",
    "            fold_length = end - start\n",
    "\n",
    "            if fold_length < self.min_fold_size:\n",
    "                continue\n",
    "\n",
    "            split = int(start + (1 - self.test_ratio) * fold_length)\n",
    "            train_idx = np.arange(start, split)\n",
    "            test_idx = np.arange(split, end)\n",
    "\n",
    "            if len(train_idx) > 100 and len(test_idx) > 50:\n",
    "                folds.append((train_idx, test_idx))\n",
    "        \n",
    "        return folds\n",
    "\n",
    "class DriftAdaptiveTimeSeriesCV:\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥ cross-validation ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ fold ‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° drift points ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡πÅ‡∏•‡∏∞ Linear Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {\n",
    "            'sequence_length': 30,\n",
    "            'units': 50,\n",
    "            'dropout_rate': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32,\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]) -> Tuple[List[float], List[float]]:\n",
    "        fold_gen = AdaptiveFoldGenerator()\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "\n",
    "        folds = fold_gen.split(X, drift_points)\n",
    "        if not folds:\n",
    "            print(\"Warning: No valid folds generated by AdaptiveFoldGenerator!\")\n",
    "            return [], []\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(folds):\n",
    "            print(f\"\\n[Adaptive Fold {i+1}] Training {self.model_type}...\")\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "            elif self.model_type == 'LINEAR':\n",
    "                model = LinearRegressionModel(**self.model_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    if len(y_pred) > 0:\n",
    "                        y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                        y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                        mae = mean_absolute_error(y_test_aligned, y_pred)\n",
    "                    else:\n",
    "                        print(f\"[Adaptive Fold {i+1}] No predictions generated (insufficient data)\")\n",
    "                        continue\n",
    "                else:\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                print(f\"[Adaptive Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                \n",
    "                metrics_rmse.append(rmse)\n",
    "                metrics_mae.append(mae)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[Adaptive Fold {i+1}] Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        return metrics_rmse, metrics_mae\n",
    "\n",
    "class BaselineTimeSeriesCV:\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥ cross-validation ‡πÅ‡∏ö‡∏ö TimeSeriesSplit ‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡πÅ‡∏•‡∏∞ Linear Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type: str = 'LSTM', model_params: dict = None, n_splits: int = 5):\n",
    "        self.model_type = model_type.upper()\n",
    "        self.model_params = model_params or {\n",
    "            'sequence_length': 30,\n",
    "            'units': 50,\n",
    "            'dropout_rate': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def run(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[float], List[float]]:\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        metrics_rmse, metrics_mae = [], []\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "            print(f\"\\n[Baseline Fold {i+1}] Training {self.model_type}...\")\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                model = RNNRegressor(model_type=self.model_type, **self.model_params)\n",
    "            elif self.model_type == 'LINEAR':\n",
    "                model = LinearRegressionModel(**self.model_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                if self.model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                    if len(y_pred) > 0:\n",
    "                        y_test_aligned = y_test.iloc[model.seq_generator.sequence_length:]\n",
    "                        y_test_aligned = y_test_aligned.iloc[:len(y_pred)]\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_aligned, y_pred))\n",
    "                        mae = mean_absolute_error(y_test_aligned, y_pred)\n",
    "                    else:\n",
    "                        print(f\"[Baseline Fold {i+1}] No predictions generated (insufficient data)\")\n",
    "                        continue\n",
    "                else:\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                print(f\"[Baseline Fold {i+1}] RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
    "                \n",
    "                metrics_rmse.append(rmse)\n",
    "                metrics_mae.append(mae)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[Baseline Fold {i+1}] Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        return metrics_rmse, metrics_mae\n",
    "\n",
    "class EnhancedModelComparison:\n",
    "    \"\"\"\n",
    "    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á RNN, LSTM, GRU ‡πÅ‡∏•‡∏∞ Linear Regression ‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, enable_tuning: bool = True, tuning_max_combinations: int = 30):\n",
    "        self.enable_tuning = enable_tuning\n",
    "        self.tuning_max_combinations = tuning_max_combinations\n",
    "        self.models = ['RNN', 'LSTM', 'GRU', 'LINEAR']\n",
    "        self.optimized_params = {}\n",
    "        self.tuning_summaries = {}\n",
    "        \n",
    "    def tune_hyperparameters(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        ‡∏ó‡∏≥ hyperparameter tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNN models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "            print(f\"\\n[TUNING] Starting hyperparameter tuning for {model_type}...\")\n",
    "            \n",
    "            tuner = HyperparameterTuner(model_type=model_type, n_splits=3)\n",
    "            best_params, best_score = tuner.grid_search(X, y, max_combinations=self.tuning_max_combinations)\n",
    "            \n",
    "            self.optimized_params[model_type] = best_params\n",
    "            self.tuning_summaries[model_type] = tuner.get_tuning_summary()\n",
    "            \n",
    "            print(f\"[TUNING] Completed tuning for {model_type}\")\n",
    "        \n",
    "        # Linear Regression ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á tune\n",
    "        self.optimized_params['LINEAR'] = {'fit_intercept': True}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HYPERPARAMETER TUNING COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    def compare_models(self, X: pd.DataFrame, y: pd.Series, drift_points: List[int]):\n",
    "        \"\"\"\n",
    "        ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢ adaptive CV ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ optimized parameters\n",
    "        \"\"\"\n",
    "        # ‡∏ó‡∏≥ hyperparameter tuning ‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "        if self.enable_tuning:\n",
    "            self.tune_hyperparameters(X, y)\n",
    "        else:\n",
    "            # ‡πÉ‡∏ä‡πâ default parameters\n",
    "            default_rnn_params = {\n",
    "                'sequence_length': 30,\n",
    "                'units': 50,\n",
    "                'dropout_rate': 0.3,\n",
    "                'learning_rate': 0.001,\n",
    "                'epochs': 50,\n",
    "                'batch_size': 32,\n",
    "                'verbose': 0\n",
    "            }\n",
    "            for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.optimized_params[model_type] = default_rnn_params\n",
    "            self.optimized_params['LINEAR'] = {'fit_intercept': True}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Testing {model_type} Model with Optimized Parameters\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            params = self.optimized_params[model_type]\n",
    "            print(f\"Using parameters: {params}\")\n",
    "            \n",
    "            # Adaptive CV\n",
    "            drift_cv = DriftAdaptiveTimeSeriesCV(model_type, params)\n",
    "            drift_rmse, drift_mae = drift_cv.run(X, y, drift_points)\n",
    "            \n",
    "            # Baseline CV\n",
    "            baseline_cv = BaselineTimeSeriesCV(model_type, params, n_splits=5)\n",
    "            base_rmse, base_mae = baseline_cv.run(X, y)\n",
    "            \n",
    "            results[model_type] = {\n",
    "                'adaptive_rmse': drift_rmse,\n",
    "                'adaptive_mae': drift_mae,\n",
    "                'baseline_rmse': base_rmse,\n",
    "                'baseline_mae': base_mae,\n",
    "                'optimized_params': params\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: dict):\n",
    "        \"\"\"\n",
    "        ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á optimized parameters\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL COMPARISON SUMMARY WITH OPTIMIZED PARAMETERS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£ tuning ‡∏Å‡πà‡∏≠‡∏ô\n",
    "        if self.enable_tuning:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                if model_type in self.tuning_summaries:\n",
    "                    print(self.tuning_summaries[model_type])\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        performance_summary = []\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                print(f\"\\n{model_type} Results:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # ‡πÅ‡∏™‡∏î‡∏á optimized parameters\n",
    "                print(f\"Optimized Parameters: {results[model_type]['optimized_params']}\")\n",
    "                \n",
    "                # Adaptive results\n",
    "                if results[model_type]['adaptive_rmse'] and results[model_type]['adaptive_mae']:\n",
    "                    avg_rmse = np.mean(results[model_type]['adaptive_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['adaptive_mae'])\n",
    "                    print(f\"Adaptive CV - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}\")\n",
    "                    \n",
    "                    performance_summary.append({\n",
    "                        'model': model_type,\n",
    "                        'adaptive_rmse': avg_rmse,\n",
    "                        'adaptive_mae': avg_mae,\n",
    "                        'params': results[model_type]['optimized_params']\n",
    "                    })\n",
    "                else:\n",
    "                    print(\"Adaptive CV - No valid results\")\n",
    "                \n",
    "                # Baseline results\n",
    "                if results[model_type]['baseline_rmse'] and results[model_type]['baseline_mae']:\n",
    "                    avg_rmse = np.mean(results[model_type]['baseline_rmse'])\n",
    "                    avg_mae = np.mean(results[model_type]['baseline_mae'])\n",
    "                    print(f\"Baseline CV - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}\")\n",
    "                else:\n",
    "                    print(\"Baseline CV - No valid results\")\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á ranking ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "        if performance_summary:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"MODEL RANKING (Based on Adaptive CV RMSE)\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            sorted_models = sorted(performance_summary, key=lambda x: x['adaptive_rmse'])\n",
    "            \n",
    "            for i, model_info in enumerate(sorted_models, 1):\n",
    "                print(f\"\\n{i}. {model_info['model']}\")\n",
    "                print(f\"   RMSE: {model_info['adaptive_rmse']:.4f}\")\n",
    "                print(f\"   MAE: {model_info['adaptive_mae']:.4f}\")\n",
    "                print(f\"   Best Parameters: {model_info['params']}\")\n",
    "        \n",
    "        # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "        best_model_info = self._find_best_model_with_params(results)\n",
    "        if best_model_info:\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"üèÜ BEST MODEL: {best_model_info['model']}\")\n",
    "            print(f\"   RMSE: {best_model_info['score']:.4f}\")\n",
    "            print(f\"   Optimal Parameters: {best_model_info['params']}\")\n",
    "            print(\"=\"*60)\n",
    "    \n",
    "    def _find_best_model_with_params(self, results: dict):\n",
    "        \"\"\"\n",
    "        ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå\n",
    "        \"\"\"\n",
    "        best_model_info = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    "        for model_type in self.models:\n",
    "            if model_type in results:\n",
    "                # ‡πÉ‡∏ä‡πâ adaptive RMSE ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
    "                if results[model_type]['adaptive_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['adaptive_rmse'])\n",
    "                    if avg_rmse < best_score:\n",
    "                        best_score = avg_rmse\n",
    "                        best_model_info = {\n",
    "                            'model': model_type,\n",
    "                            'score': avg_rmse,\n",
    "                            'params': results[model_type]['optimized_params']\n",
    "                        }\n",
    "                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ adaptive results ‡πÉ‡∏ä‡πâ baseline\n",
    "                elif results[model_type]['baseline_rmse']:\n",
    "                    avg_rmse = np.mean(results[model_type]['baseline_rmse'])\n",
    "                    if avg_rmse < best_score:\n",
    "                        best_score = avg_rmse\n",
    "                        best_model_info = {\n",
    "                            'model': model_type,\n",
    "                            'score': avg_rmse,\n",
    "                            'params': results[model_type]['optimized_params']\n",
    "                        }\n",
    "        \n",
    "        return best_model_info\n",
    "    \n",
    "    def get_best_params_for_model(self, model_type: str):\n",
    "        \"\"\"\n",
    "        ‡∏î‡∏∂‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "        \"\"\"\n",
    "        if model_type.upper() in self.optimized_params:\n",
    "            return self.optimized_params[model_type.upper()]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Detect drift points\n",
    "    print(\"Starting Drift Detection...\")\n",
    "    detector = DriftPointDetector(\n",
    "        window_size=120,\n",
    "        threshold=0.001,  \n",
    "        step_size=30,\n",
    "        min_effect_size=0.3,\n",
    "        stability_window=60,\n",
    "        confirmation_tests=2\n",
    "    )\n",
    "    drift_points = detector.detect(X)\n",
    "    print(f\"Detected {len(drift_points)} drift points\")\n",
    "    \n",
    "    # 2) ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° hyperparameter tuning\n",
    "    print(\"\\nStarting Enhanced Model Comparison with Hyperparameter Tuning...\")\n",
    "    \n",
    "    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô hyperparameter tuning (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)\n",
    "    enhanced_comparator = EnhancedModelComparison(\n",
    "        enable_tuning=True, \n",
    "        tuning_max_combinations=25  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤\n",
    "    )\n",
    "    \n",
    "    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ default parameters (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)\n",
    "    # enhanced_comparator = EnhancedModelComparison(enable_tuning=False)\n",
    "    \n",
    "    results = enhanced_comparator.compare_models(X, y, drift_points)\n",
    "    enhanced_comparator.print_summary(results)\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST PARAMETERS FOR EACH MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_type in ['RNN', 'LSTM', 'GRU', 'LINEAR']:\n",
    "        best_params = enhanced_comparator.get_best_params_for_model(model_type)\n",
    "        if best_params:\n",
    "            print(f\"\\n{model_type} Best Parameters:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á drift point index ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö ‡∏ß‡∏±‡∏ô/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô/‡∏õ‡∏µ\n",
    "    drift_dates_formatted = df.iloc[drift_points]['Date'].dt.strftime('%d/%m/%Y').tolist()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DRIFT DETECTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Detected Drift Points: {len(drift_points)}\")\n",
    "    print(\"Drift Dates:\")\n",
    "    for i, date in enumerate(drift_dates_formatted, 1):\n",
    "        print(f\"  {i}. {date}\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
